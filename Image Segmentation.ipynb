{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ImageSegmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CY8pqYLAkRtU",
        "jKN6s1zoklqB",
        "bCQuuwSikoA5",
        "mH3lzlVqnFI2",
        "KOpdDojOgmz8",
        "wyU6nJCDyXDV",
        "eetgWI7hgV3J"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fQMjNHmG6Rp"
      },
      "source": [
        "\r\n",
        "# Homework 2 - Image Segmentation\r\n",
        "The notebook is divided into several sections:\r\n",
        "\r\n",
        "- Setup - Importing libraries, defining the rle_encode function, mounting Drive and unzipping the dataset in the proper Drive directory. Indeed, the notebook was created using the Drive integration with Colab, therefore the main directory is the folder /AN2DL/ImageSegmentation, which was created in advance with the dataset in it.\r\n",
        "- Data preparation:\r\n",
        "  - Organizing dataset folders - Masks are converted into integer labels. Additionally, the images and the masks of all the teams are put in a single folder, and the same is done for the test images. The resulting dataset structure is explained at the beginning of the section.\r\n",
        "  - Creating the generators and the custom dataset class - The generators for data augmentation are created, and the CustomDataset class is defined to be used later on in the models.\r\n",
        "- Models:\r\n",
        "  - First Model (F-CNN)\r\n",
        "  - Second Model (U-NET)\r\n",
        "  - Third Model (VGG-16)\r\n",
        "\r\n",
        "  In each model section, the dataset objects are created, the architecture is defined, the optimization parameters are set, the callbacks are created, the model is trained and finally the predictions on the test set are computed, exporting the results in a csv format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY8pqYLAkRtU"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOBDSA9BnFIx"
      },
      "source": [
        "# Importing the necessary libraries and setting the seed(s) to make the code replicable\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3csqSyMY0ohq"
      },
      "source": [
        "# Defining the rle_encode function, which is used to correctly format the output submission.json file in the prediction phase.\r\n",
        "def rle_encode(img):\r\n",
        "    '''\r\n",
        "    img: numpy array, 1 - foreground, 0 - background\r\n",
        "    Returns run length as string formatted\r\n",
        "    '''\r\n",
        "    pixels = img.flatten()\r\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\r\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\r\n",
        "    runs[1::2] -= runs[::2]\r\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bhsWQnnU-i"
      },
      "source": [
        "# Mounting Drive to Colab, as the Drive folder /AN2DL/ImageSegmentation is the main directory for this homework\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bns18Nm7oORk"
      },
      "source": [
        "# Unzipping the dataset (named \"Development_Dataset.zip\"), which has to be previously put in the homework directory\r\n",
        "!unzip '/content/drive/My Drive/AN2DL/ImageSegmentation/Development_Dataset.zip'\r\n",
        "\r\n",
        "# Saving the directories for the dataset, the training set and the test set (to be used later)\r\n",
        "cwd = os.getcwd()                                                               # This is the current working directory, in which the dataset has been unzipped\r\n",
        "dataset_dir = os.path.join(cwd, 'Development_Dataset')                          # This is the dataset directory, which contains the training and the test folders, along with the json\r\n",
        "training_dir = os.path.join(dataset_dir, 'Training')                            # This is the training directory, which contains the training samples\r\n",
        "test_dir = os.path.join(dataset_dir, 'Test_Dev')                                # This is the test directory, which contains the test samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKN6s1zoklqB"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCQuuwSikoA5"
      },
      "source": [
        "## Organizing dataset folders\r\n",
        "Here the masks images are converted into integer labels. Additionally, the dataset folder structure is reorganized as follows:\r\n",
        "  - Training\r\n",
        "    - Images\r\n",
        "    - Masks\r\n",
        "  - Test_Dev\r\n",
        "    - Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLUOxeBTSAR"
      },
      "source": [
        "# Defining the read_rgb_mask function, which will be used to convert the RGB masks into integer labels\r\n",
        "def read_rgb_mask(img_path):\r\n",
        "    '''\r\n",
        "    img_path: path to the mask file\r\n",
        "    Returns the numpy array containing target values\r\n",
        "    '''\r\n",
        "\r\n",
        "    mask_img = Image.open(img_path)\r\n",
        "    mask_arr = np.array(mask_img)\r\n",
        "\r\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\r\n",
        "  \r\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\r\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [0, 0, 0], axis=-1))] = 0\r\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [254, 124, 18], axis=-1))] = 0\r\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\r\n",
        "\r\n",
        "    return new_mask_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sjJXMHtTZCC"
      },
      "source": [
        "# Reading the RGB masks from the different teams and transforming them into integer labels\r\n",
        "# The resulting labels are saved into the Training/Masks folder.\r\n",
        "target_dir = os.path.join(cwd, training_dir, 'Masks')\r\n",
        "if not os.path.exists(target_dir):\r\n",
        "    os.makedirs(target_dir)\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Bipbip/Mais/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Bipbip/Mais/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Bipbip/Haricot/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Bipbip/Haricot/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Pead/Mais/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Pead/Mais/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Pead/Haricot/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Pead/Haricot/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Roseau/Mais/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Roseau/Mais/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Roseau/Haricot/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Roseau/Haricot/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))\r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Weedelec/Mais/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Weedelec/Mais/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))     \r\n",
        "\r\n",
        "for f in os.listdir(os.path.join(training_dir, 'Weedelec/Haricot/Masks')):\r\n",
        "  mask = read_rgb_mask(os.path.join(training_dir, 'Weedelec/Haricot/Masks', f))\r\n",
        "  mask_img = Image.fromarray(mask, 'P')\r\n",
        "  mask_img.save(os.path.join(target_dir, f))       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdfafy_tKiX9"
      },
      "source": [
        "# Moving the training images from the different teams into the new folder Training/Images\r\n",
        "target_dir = os.path.join(training_dir, 'Images')\r\n",
        "if not os.path.exists(target_dir):\r\n",
        "    os.makedirs(target_dir)\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Bipbip/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Bipbip/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Pead/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Pead/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)  \r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Roseau/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    img = Image.open(os.path.join(training_dir, 'Roseau/Mais/Images', file_name))\r\n",
        "    img.save(os.path.join(target_dir, file_name[:-4] + \".jpg\"), \"JPEG\")\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Roseau/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    img = Image.open(os.path.join(training_dir, 'Roseau/Haricot/Images', file_name))\r\n",
        "    img.save(os.path.join(target_dir, file_name[:-4] + \".jpg\"), \"JPEG\")\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Weedelec/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir = os.path.join(training_dir, 'Weedelec/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir):\r\n",
        "    shutil.move(os.path.join(source_dir, file_name), target_dir)         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydrah2yKcx6u"
      },
      "source": [
        "# Deleting the now useless folder, since images and masks have been moved into the new folder.\r\n",
        "# This is not mandatory, but it is done just for clarity since there are many folders with the same names along the dataset.\r\n",
        "\r\n",
        "# Deleting the now useless folder Training/Bipbip\r\n",
        "shutil.rmtree(os.path.join(training_dir, 'Bipbip'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Training/Pead\r\n",
        "shutil.rmtree(os.path.join(training_dir, 'Pead'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Training/Roseau\r\n",
        "shutil.rmtree(os.path.join(training_dir, 'Roseau'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Training/Weedelec\r\n",
        "shutil.rmtree(os.path.join(training_dir, 'Weedelec'), ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLC54k9CJwQ9"
      },
      "source": [
        "# Creating the text files for the training-validation split\r\n",
        "train_file = open(\"/content/Development_Dataset/train.txt\", \"a\")\r\n",
        "val_file = open(\"/content/Development_Dataset/val.txt\", \"a\")\r\n",
        "\r\n",
        "import random\r\n",
        "random.seed(1234)\r\n",
        "file_names = os.listdir('/content/Development_Dataset/Training/Images')\r\n",
        "random.shuffle(file_names)\r\n",
        "\r\n",
        "for i in range(0, len(file_names)):\r\n",
        "  if(i == ((0.8*len(file_names))-1) or i == (len(file_names)-1)):\r\n",
        "    file_to_write = str(file_names[i][:-4])\r\n",
        "  else:\r\n",
        "    file_to_write = str(file_names[i][:-4] + \"\\n\")\r\n",
        "  if(i < 0.8*len(file_names)):\r\n",
        "    train_file.write(file_to_write)\r\n",
        "  else:\r\n",
        "    val_file.write(file_to_write)\r\n",
        "\r\n",
        "train_file.close()\r\n",
        "val_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soRk8IvhpkIb"
      },
      "source": [
        "# Moving the Test_Dev images of every team into a single folder, which is Test_Dev/Images.\r\n",
        "# This is done because the submission.json file must be filled with the prediction over all the Test_Dev images.\r\n",
        "# Hence, for simplicity, all the images are moved into that single folder.\r\n",
        "target_dir = os.path.join(test_dir, 'Images')\r\n",
        "if not os.path.exists(target_dir):\r\n",
        "    os.makedirs(target_dir)\r\n",
        "\r\n",
        "source_dir_1 = os.path.join(test_dir, 'Bipbip/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir_1):\r\n",
        "    shutil.move(os.path.join(source_dir_1, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir_2 = os.path.join(test_dir, 'Bipbip/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir_2):\r\n",
        "    shutil.move(os.path.join(source_dir_2, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir_3 = os.path.join(test_dir, 'Pead/Mais/Images')\r\n",
        "for file_name in os.listdir(source_dir_3):\r\n",
        "    shutil.move(os.path.join(source_dir_3, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir_4 = os.path.join(test_dir, 'Pead/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir_4):\r\n",
        "    shutil.move(os.path.join(source_dir_4, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir_5 = os.path.join(test_dir, 'Roseau/Mais/Images')    \r\n",
        "for file_name in os.listdir(source_dir_5):\r\n",
        "    img = Image.open(os.path.join(source_dir_5, file_name))\r\n",
        "    img.save(os.path.join(target_dir, file_name[:-4] + \".jpg\"), \"JPEG\")\r\n",
        "\r\n",
        "source_dir_6 = os.path.join(test_dir, 'Roseau/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir_6):\r\n",
        "    img = Image.open(os.path.join(source_dir_6, file_name))\r\n",
        "    img.save(os.path.join(target_dir, file_name[:-4] + \".jpg\"), \"JPEG\")\r\n",
        "\r\n",
        "source_dir_7 = os.path.join(test_dir, 'Weedelec/Mais/Images')    \r\n",
        "for file_name in os.listdir(source_dir_7):\r\n",
        "    shutil.move(os.path.join(source_dir_7, file_name), target_dir)\r\n",
        "\r\n",
        "source_dir_8 = os.path.join(test_dir, 'Weedelec/Haricot/Images')\r\n",
        "for file_name in os.listdir(source_dir_8):\r\n",
        "    shutil.move(os.path.join(source_dir_8, file_name), target_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk8iPtX52JHB"
      },
      "source": [
        "# Deleting the now useless folder, since images and masks have been moved into the new folder.\r\n",
        "# This is not mandatory, but it is done just for clarity since there are many folders with the same names along the dataset.\r\n",
        "\r\n",
        "# Deleting the now useless folder Test_Dev/Bipbip\r\n",
        "shutil.rmtree(os.path.join(test_dir, 'Bipbip'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Test_Dev/Pead\r\n",
        "shutil.rmtree(os.path.join(test_dir, 'Pead'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Test_Dev/Roseau\r\n",
        "shutil.rmtree(os.path.join(test_dir, 'Roseau'), ignore_errors=True)\r\n",
        "# Deleting the now useless folder Test_Dev/Weedelec\r\n",
        "shutil.rmtree(os.path.join(test_dir, 'Weedelec'), ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3lzlVqnFI2"
      },
      "source": [
        "## Creating the generators and the custom dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N88wG50nFI3"
      },
      "source": [
        "# Creating the ImageDataGenerator objects - one for images and one for masks - performing data augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "img_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                  width_shift_range=10,\n",
        "                                  height_shift_range=10,\n",
        "                                  zoom_range=0.3,\n",
        "                                  horizontal_flip=True,\n",
        "                                  vertical_flip=True,\n",
        "                                  fill_mode='reflect')\n",
        "mask_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                   width_shift_range=10,\n",
        "                                   height_shift_range=10,\n",
        "                                   zoom_range=0.3,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                   fill_mode='reflect')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsieZk4aKhm6"
      },
      "source": [
        "# Defining the CustomDataset class, which will be used inside each model section to create the training and validation datasets\n",
        "# This implementation is taken from the lab notebook, adjusting some details\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
        "               preprocessing_function=None, out_shape=None):\n",
        "    if which_subset == 'training':\n",
        "      subset_file = os.path.join(dataset_dir, 'train.txt')\n",
        "    elif which_subset == 'validation':\n",
        "      subset_file = os.path.join(dataset_dir, 'val.txt')\n",
        "    \n",
        "    with open(subset_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    \n",
        "    subset_filenames = []\n",
        "    for line in lines:\n",
        "      subset_filenames.append(line.strip()) \n",
        "\n",
        "    self.which_subset = which_subset\n",
        "    self.dataset_dir = dataset_dir\n",
        "    self.subset_filenames = subset_filenames\n",
        "    self.img_generator = img_generator\n",
        "    self.mask_generator = mask_generator\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.subset_filenames)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    curr_filename = self.subset_filenames[index]\n",
        "    img = Image.open(os.path.join(self.dataset_dir, 'Training/Images', curr_filename + '.jpg'))\n",
        "    mask = Image.open(os.path.join(self.dataset_dir, 'Training/Masks', curr_filename + '.png'))\n",
        "\n",
        "    img = img.resize(self.out_shape)\n",
        "    mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n",
        "    \n",
        "    img_arr = np.array(img)\n",
        "    mask_arr = np.array(mask)\n",
        "    mask_arr = np.expand_dims(mask_arr, -1)\n",
        "\n",
        "    if self.which_subset == 'training':\n",
        "      if self.img_generator is not None and self.mask_generator is not None:\n",
        "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
        "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
        "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "\n",
        "        out_mask = np.zeros_like(mask_arr)\n",
        "        for c in np.unique(mask_arr):\n",
        "          if c > 0:\n",
        "            curr_class_arr = np.float32(mask_arr == c)\n",
        "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
        "            curr_class_arr = np.uint8(curr_class_arr)\n",
        "            curr_class_arr = curr_class_arr * c \n",
        "            out_mask += curr_class_arr\n",
        "    else:\n",
        "      out_mask = mask_arr\n",
        "    \n",
        "    if self.preprocessing_function is not None:\n",
        "        img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    return img_arr, np.float32(out_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOpdDojOgmz8"
      },
      "source": [
        "# F-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyrdiIh_PWjB"
      },
      "source": [
        "# Creating the CustomDataset objects for the model, followed by the actual dataset that will be used for segmentation\n",
        "\n",
        "# Image dimensions to work with\n",
        "img_h = 512\n",
        "img_w = 512\n",
        "\n",
        "# Batch size\n",
        "bs = 4\n",
        "\n",
        "# Training\n",
        "dataset = CustomDataset('/content/Development_Dataset', 'training', \n",
        "                        img_generator=img_data_gen, mask_generator=mask_data_gen, out_shape=[img_h, img_w])\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n",
        "\n",
        "train_dataset = train_dataset.batch(bs)                                                                 \n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# Validation\n",
        "dataset_valid = CustomDataset('/content/Development_Dataset', 'validation', out_shape=[img_h, img_w])\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\n",
        "\n",
        "valid_dataset = valid_dataset.batch(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0G22dxogpAp"
      },
      "source": [
        "# Defining the model structure\r\n",
        "# This is taken by the lab notebook: there is a simple encoding + decoding structure\r\n",
        "# The encoder is composed by a convolutional a ReLU, and a maxpooling layers for each depth level\r\n",
        "# The intermediate bottleneck is composed only by a convolution and a ReLU, and the result is given to the decoder\r\n",
        "# The decoder is composed by an upsampling, a convolutional and a ReLU layers for each depth level\r\n",
        "# Finally, the prediction is done using 3 filters (as the number of classes) and a SoftMax activation function\r\n",
        "def create_model(depth, start_f, num_classes, dynamic_input_shape):\r\n",
        "\r\n",
        "    model = tf.keras.Sequential()\r\n",
        "    \r\n",
        "    # Encoder\r\n",
        "    for i in range(depth):\r\n",
        "        \r\n",
        "        if i == 0:\r\n",
        "            if dynamic_input_shape:\r\n",
        "                input_shape = [None, None, 3]\r\n",
        "            else:\r\n",
        "                input_shape = [img_h, img_w, 3]\r\n",
        "        else:\r\n",
        "            input_shape=[None]\r\n",
        "        \r\n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f, \r\n",
        "                                         kernel_size=(3, 3),\r\n",
        "                                         strides=(1, 1),\r\n",
        "                                         padding='same',\r\n",
        "                                         input_shape=input_shape))\r\n",
        "        model.add(tf.keras.layers.ReLU())\r\n",
        "        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "        start_f *= 2\r\n",
        "\r\n",
        "    # Bottleneck\r\n",
        "    model.add(tf.keras.layers.Conv2D(filters=start_f, kernel_size=(3, 3), strides=(1, 1), padding='same'))\r\n",
        "    model.add(tf.keras.layers.ReLU())\r\n",
        "    \r\n",
        "    start_f = start_f // 2\r\n",
        "        \r\n",
        "    # Decoder\r\n",
        "    for i in range(depth):\r\n",
        "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\r\n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f,\r\n",
        "                                         kernel_size=(3, 3),\r\n",
        "                                         strides=(1, 1),\r\n",
        "                                         padding='same'))\r\n",
        "        model.add(tf.keras.layers.ReLU())\r\n",
        "\r\n",
        "        start_f = start_f // 2\r\n",
        "\r\n",
        "    # Prediction Layer\r\n",
        "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\r\n",
        "                                     kernel_size=(1, 1),\r\n",
        "                                     strides=(1, 1),\r\n",
        "                                     padding='same',\r\n",
        "                                     activation='softmax'))\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uLTU-_jgreV"
      },
      "source": [
        "# Creating the model and visualizing a summary of it\r\n",
        "model = create_model(depth=5, start_f=8, num_classes=3, dynamic_input_shape=False)\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuRY_V-qguEl"
      },
      "source": [
        "# Optimization parameters\r\n",
        "# The custom validation metric is the mean intersection over union defined in the lab notebook\r\n",
        "\r\n",
        "# Loss function\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "# Learning rate and optimizer\r\n",
        "lr = 1e-3\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "\r\n",
        "# Validation metrics\r\n",
        "def meanIoU(y_true, y_pred):\r\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n",
        "\r\n",
        "    per_class_iou = []\r\n",
        "\r\n",
        "    for i in range(1,3): # Excluding the background class 0\r\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\r\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n",
        "    \r\n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\r\n",
        "      per_class_iou.append(iou)\r\n",
        "\r\n",
        "    return tf.reduce_mean(per_class_iou)\r\n",
        "    \r\n",
        "metrics = ['accuracy', meanIoU]\r\n",
        "\r\n",
        "# Compile Model\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_PAhKMvgw3C"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\r\n",
        "# The purpose of this piece of code is to create a \"multiclass_segmentationexperiments\" folder inside the directory of this homework (if not already created).\r\n",
        "# Inside it, it creates a folder called \"F-CNN_\" followed by the date and the time of execution, to recognize the experiment.\r\n",
        "# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in meanIoU on the Validation set.\r\n",
        "# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\r\n",
        "# Finally, the model is fitted using the training and validation data defined before.\r\n",
        "\r\n",
        "# Creating the \"multiclass_segmentation_experiments\" folder if not already created\r\n",
        "exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/ImageSegmentation/', 'multiclass_segmentation_experiments')\r\n",
        "if not os.path.exists(exps_dir):\r\n",
        "    os.makedirs(exps_dir)\r\n",
        "\r\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n",
        "\r\n",
        "# Creating the folder in which the model weights will be saved\r\n",
        "model_name = 'F-CNN'\r\n",
        "\r\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n",
        "if not os.path.exists(exp_dir):\r\n",
        "    os.makedirs(exp_dir)\r\n",
        "\r\n",
        "# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation meanIoU    \r\n",
        "callbacks = []\r\n",
        "\r\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n",
        "if not os.path.exists(ckpt_dir):\r\n",
        "    os.makedirs(ckpt_dir)\r\n",
        "\r\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \r\n",
        "                                                   monitor='val_meanIoU',\r\n",
        "                                                   mode='max',\r\n",
        "                                                   verbose=0,\r\n",
        "                                                   save_best_only=True,\r\n",
        "                                                   save_weights_only=True)\r\n",
        "callbacks.append(ckpt_callback)\r\n",
        "\r\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\r\n",
        "early_stop = True\r\n",
        "if early_stop:\r\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\r\n",
        "    callbacks.append(es_callback)\r\n",
        "\r\n",
        "\r\n",
        "# Fitting the model\r\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\r\n",
        "model.fit(x=train_dataset,\r\n",
        "          epochs=100,\r\n",
        "          steps_per_epoch=(len(dataset) // bs),\r\n",
        "          validation_data=valid_dataset,\r\n",
        "          validation_steps=(len(dataset_valid) // bs), \r\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emRECYVpO70E"
      },
      "source": [
        "# Loading the best weights of the trained model\r\n",
        "full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageSegmentation/multiclass_segmentation_experiments', exp_dir)\r\n",
        "latest = tf.train.latest_checkpoint(full_path)\r\n",
        "model.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkHMh39eJzV"
      },
      "source": [
        "# Checking how the model predictions on the validation set\r\n",
        "import time\r\n",
        "from matplotlib import cm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Assigning a color to each class\r\n",
        "evenly_spaced_interval = np.linspace(0, 1, 20)\r\n",
        "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\r\n",
        "\r\n",
        "iterator = iter(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvGBP2pQNiLu"
      },
      "source": [
        "# Visualizing an image in the validation set, its mask and its model prediction\r\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\r\n",
        "fig.show()\r\n",
        "image, target = next(iterator)\r\n",
        "\r\n",
        "image = image[0]\r\n",
        "target = target[0, ..., 0]\r\n",
        "\r\n",
        "out_sigmoid = model.predict(x=tf.expand_dims(image, 0))\r\n",
        "predicted_class = tf.argmax(out_sigmoid, -1)\r\n",
        "\r\n",
        "predicted_class = predicted_class[0, ...]\r\n",
        "\r\n",
        "target_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n",
        "prediction_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n",
        "\r\n",
        "target_img[np.where(target == 0)] = [0, 0, 0]\r\n",
        "for i in range(1, 3):\r\n",
        "  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\r\n",
        "\r\n",
        "prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\r\n",
        "for i in range(1, 3):\r\n",
        "  prediction_img[np.where(predicted_class == i)] = np.array(colors[i-1])[:3] * 255\r\n",
        "\r\n",
        "ax[0].imshow(np.uint8(image))\r\n",
        "ax[1].imshow(np.uint8(target_img))\r\n",
        "ax[2].imshow(np.uint8(prediction_img))\r\n",
        "\r\n",
        "fig.canvas.draw()\r\n",
        "time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsk06uoHgzHD"
      },
      "source": [
        "# Computing the prediction on the Test_Dev images and generating the submission.json file\r\n",
        "submission_dict = {}\r\n",
        "\r\n",
        "# Storing the names of the images\r\n",
        "test_img_dir = os.path.join(test_dir, 'Images')\r\n",
        "file_names_img = os.listdir(test_img_dir)\r\n",
        "\r\n",
        "# For each image, we resize it to the dimensions on which the model has been trained\r\n",
        "# We give it as input to the model and then we resize the model output to match the original image size (according to the team it belongs)\r\n",
        "# Finally, we compute the argmax to get the predicted class for each pixel and we fill the submission_dict according to the requested structure and encoding\r\n",
        "for i in file_names_img:\r\n",
        "  image = Image.open(os.path.join(test_img_dir, i)).convert('RGB')\r\n",
        "  image = image.resize((img_h, img_w))\r\n",
        "  img_array = np.array(image)\r\n",
        "  out_softmax = model.predict(tf.expand_dims(img_array, 0))\r\n",
        "\r\n",
        "  if (i.split('_')[0] == 'Bipbip'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (1536, 2048), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Pead'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (2464, 3280), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Weedelec'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (3456, 5184), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Roseau'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (820, 1225), method='nearest')\r\n",
        "  \r\n",
        "  predicted_class = tf.argmax(resized_out_softmax, -1)\r\n",
        "  predicted_class = np.array(predicted_class)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]] = {}\r\n",
        "  submission_dict[i[:-4]]['shape'] = predicted_class.shape\r\n",
        "  submission_dict[i[:-4]]['team'] = i.split('_')[0]\r\n",
        "  submission_dict[i[:-4]]['crop'] = i.split('_')[1][0].upper() + i.split('_')[1][1:]\r\n",
        "  submission_dict[i[:-4]]['segmentation'] = {}\r\n",
        "\r\n",
        "  rle_encoded_crop = rle_encode(predicted_class == 1)\r\n",
        "  rle_encoded_weed = rle_encode(predicted_class == 2)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]]['segmentation']['crop'] = rle_encoded_crop\r\n",
        "  submission_dict[i[:-4]]['segmentation']['weed'] = rle_encoded_weed\r\n",
        "\r\n",
        "# Exporting the submission_dict created into a submission.json file\r\n",
        "with open('/content/drive/My Drive/AN2DL/ImageSegmentation/submission.json', 'w') as f:\r\n",
        "    json.dump(submission_dict, f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyU6nJCDyXDV"
      },
      "source": [
        "# U-NET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mglG1rVAe6Mv"
      },
      "source": [
        "# Creating the CustomDataset objects for the model, followed by the actual dataset that will be used for segmentation\r\n",
        "# Note that the images are preprocessed using the VGG-16 preprocess_input function\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "\r\n",
        "# Image dimensions to work with\r\n",
        "img_h = 768\r\n",
        "img_w = 768\r\n",
        "\r\n",
        "# Batch size\r\n",
        "bs = 4\r\n",
        "\r\n",
        "# Training\r\n",
        "dataset = CustomDataset('/content/Development_Dataset', 'training', \r\n",
        "                        img_generator=img_data_gen, mask_generator=mask_data_gen,\r\n",
        "                        preprocessing_function=preprocess_input, out_shape=[img_h, img_w])\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "\r\n",
        "train_dataset = train_dataset.batch(bs)                                                                 \r\n",
        "train_dataset = train_dataset.repeat()\r\n",
        "\r\n",
        "# Validation\r\n",
        "dataset_valid = CustomDataset('/content/Development_Dataset', 'validation',\r\n",
        "                              preprocessing_function=preprocess_input, out_shape=[img_h, img_w])\r\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "\r\n",
        "valid_dataset = valid_dataset.batch(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDTx7XRwyaIn"
      },
      "source": [
        "# Defining the model structure\r\n",
        "# This is the U-Net Xception-style model - Taken from https://keras.io/examples/vision/oxford_pets_image_segmentation/\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "def get_model(img_size, num_classes):\r\n",
        "    inputs = tf.keras.Input(shape=img_size + (3,))\r\n",
        "\r\n",
        "    ### [First half of the network: downsampling inputs] ###\r\n",
        "\r\n",
        "    # Entry block\r\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\r\n",
        "    x = layers.BatchNormalization()(x)\r\n",
        "    x = layers.Activation(\"relu\")(x)\r\n",
        "\r\n",
        "    previous_block_activation = x  # Set aside residual\r\n",
        "\r\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\r\n",
        "    for filters in [64, 128, 256]:\r\n",
        "        x = layers.Activation(\"relu\")(x)\r\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\r\n",
        "        x = layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "        x = layers.Activation(\"relu\")(x)\r\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\r\n",
        "        x = layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\r\n",
        "\r\n",
        "        # Project residual\r\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\r\n",
        "            previous_block_activation\r\n",
        "        )\r\n",
        "        x = layers.add([x, residual])  # Add back residual\r\n",
        "        previous_block_activation = x  # Set aside next residual\r\n",
        "\r\n",
        "    ### [Second half of the network: upsampling inputs] ###\r\n",
        "\r\n",
        "    for filters in [256, 128, 64, 32]:\r\n",
        "        x = layers.Activation(\"relu\")(x)\r\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\r\n",
        "        x = layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "        x = layers.Activation(\"relu\")(x)\r\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\r\n",
        "        x = layers.BatchNormalization()(x)\r\n",
        "\r\n",
        "        x = layers.UpSampling2D(2)(x)\r\n",
        "\r\n",
        "        # Project residual\r\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\r\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\r\n",
        "        x = layers.add([x, residual])  # Add back residual\r\n",
        "        previous_block_activation = x  # Set aside next residual\r\n",
        "\r\n",
        "    # Add a per-pixel classification layer\r\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\r\n",
        "\r\n",
        "    # Define the model\r\n",
        "    model = tf.keras.Model(inputs, outputs)\r\n",
        "    return model\r\n",
        "\r\n",
        "# Free up RAM in case the model definition cells were run multiple times\r\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gw1QIiMygsD"
      },
      "source": [
        "# Creating the model and visualizing a summary of it\r\n",
        "model_2 = get_model((img_h, img_w), 3)\r\n",
        "\r\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGexhrkuyinz"
      },
      "source": [
        "# Optimization parameters\r\n",
        "# The custom validation metric is the mean intersection over union defined in the lab notebook\r\n",
        "\r\n",
        "# Loss function\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "# Learning rate and optimizer\r\n",
        "lr = 1e-4\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "\r\n",
        "# Validation metrics\r\n",
        "def meanIoU(y_true, y_pred):\r\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n",
        "\r\n",
        "    per_class_iou = []\r\n",
        "\r\n",
        "    for i in range(1,3): # Excluding the background class 0\r\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\r\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n",
        "    \r\n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\r\n",
        "      per_class_iou.append(iou)\r\n",
        "\r\n",
        "    return tf.reduce_mean(per_class_iou)\r\n",
        "\r\n",
        "metrics = ['accuracy', meanIoU]\r\n",
        "\r\n",
        "# Compile Model\r\n",
        "model_2.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAuNsr7eynKE"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\r\n",
        "# The purpose of this piece of code is to create a \"multiclass_segmentationexperiments\" folder inside the directory of this homework (if not already created).\r\n",
        "# Inside it, it creates a folder called \"U-NET_\" followed by the date and the time of execution, to recognize the experiment.\r\n",
        "# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in meanIoU on the Validation set.\r\n",
        "# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\r\n",
        "# Finally, the model is fitted using the training and validation data defined before.\r\n",
        "\r\n",
        "# Creating the \"multiclass_segmentation_experiments\" folder if not already created\r\n",
        "exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/ImageSegmentation/', 'multiclass_segmentation_experiments')\r\n",
        "if not os.path.exists(exps_dir):\r\n",
        "    os.makedirs(exps_dir)\r\n",
        "\r\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\r\n",
        "\r\n",
        "# Creating the folder in which the model weights will be saved\r\n",
        "model_name = 'U_NET'\r\n",
        "\r\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\r\n",
        "if not os.path.exists(exp_dir):\r\n",
        "    os.makedirs(exp_dir)\r\n",
        "\r\n",
        "# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation meanIoU  \r\n",
        "callbacks_2 = []\r\n",
        "\r\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\r\n",
        "if not os.path.exists(ckpt_dir):\r\n",
        "    os.makedirs(ckpt_dir)\r\n",
        "\r\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \r\n",
        "                                                   monitor='val_meanIoU',\r\n",
        "                                                   mode='max',\r\n",
        "                                                   verbose=0,\r\n",
        "                                                   save_best_only=True,\r\n",
        "                                                   save_weights_only=True)\r\n",
        "callbacks_2.append(ckpt_callback)\r\n",
        "\r\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\r\n",
        "early_stop = True\r\n",
        "if early_stop:\r\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\r\n",
        "    callbacks_2.append(es_callback)\r\n",
        "\r\n",
        "# Fitting the model\r\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier\r\n",
        "model_2.fit(x=train_dataset,\r\n",
        "            epochs=100,\r\n",
        "            steps_per_epoch=(len(dataset) // bs),\r\n",
        "            validation_data=valid_dataset,\r\n",
        "            validation_steps=(len(dataset_valid) // bs), \r\n",
        "            callbacks=callbacks_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCjcNzq_fsLy"
      },
      "source": [
        "# Loading the best weights of the trained model\r\n",
        "full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageSegmentation/multiclass_segmentation_experiments', exp_dir)\r\n",
        "latest = tf.train.latest_checkpoint(full_path)\r\n",
        "model_2.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVdIDaL_fupj"
      },
      "source": [
        "# Checking how the model predictions on the validation set\r\n",
        "import time\r\n",
        "from matplotlib import cm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Assigning a color to each class\r\n",
        "evenly_spaced_interval = np.linspace(0, 1, 20)\r\n",
        "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\r\n",
        "\r\n",
        "iterator = iter(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgRNjT5mfxfe"
      },
      "source": [
        "# Visualizing an image in the validation set, its mask and its model prediction\r\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\r\n",
        "fig.show()\r\n",
        "image, target = next(iterator)\r\n",
        "\r\n",
        "image = image[0]\r\n",
        "target = target[0, ..., 0]\r\n",
        "\r\n",
        "out_sigmoid = model_2.predict(x=tf.expand_dims(image, 0))\r\n",
        "predicted_class = tf.argmax(out_sigmoid, -1)\r\n",
        "\r\n",
        "predicted_class = predicted_class[0, ...]\r\n",
        "\r\n",
        "target_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n",
        "prediction_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n",
        "\r\n",
        "target_img[np.where(target == 0)] = [0, 0, 0]\r\n",
        "for i in range(1, 3):\r\n",
        "  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\r\n",
        "\r\n",
        "prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\r\n",
        "for i in range(1, 3):\r\n",
        "  prediction_img[np.where(predicted_class == i)] = np.array(colors[i-1])[:3] * 255\r\n",
        "\r\n",
        "ax[0].imshow(np.uint8(image))\r\n",
        "ax[1].imshow(np.uint8(target_img))\r\n",
        "ax[2].imshow(np.uint8(prediction_img))\r\n",
        "\r\n",
        "fig.canvas.draw()\r\n",
        "time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mPY1bHnys_O"
      },
      "source": [
        "# Computing the prediction on the Test_Dev images and generating the submission.json file\r\n",
        "submission_dict = {}\r\n",
        "\r\n",
        "# Storing the names of the images\r\n",
        "test_img_dir = os.path.join(test_dir, 'Images')\r\n",
        "file_names_img = os.listdir(test_img_dir)\r\n",
        "\r\n",
        "# For each image, we resize it to the dimensions on which the model has been trained and we apply the preprocess_input function on it\r\n",
        "# We give it as input to the model and then we resize the model output to match the original image size (according to the team it belongs)\r\n",
        "# Finally, we compute the argmax to get the predicted class for each pixel and we fill the submission_dict according to the requested structure and encoding\r\n",
        "for i in file_names_img:\r\n",
        "  image = Image.open(os.path.join(test_img_dir, i)).convert('RGB')\r\n",
        "  image = image.resize((img_h, img_w))\r\n",
        "  img_array = np.array(image)\r\n",
        "  img_array = preprocess_input(img_array)\r\n",
        "  out_softmax = model_2.predict(tf.expand_dims(img_array, 0))\r\n",
        "\r\n",
        "  if (i.split('_')[0] == 'Bipbip'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (1536, 2048), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Pead'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (2464, 3280), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Weedelec'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (3456, 5184), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Roseau'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (820, 1225), method='nearest')\r\n",
        " \r\n",
        "  predicted_class = tf.argmax(resized_out_softmax, -1)\r\n",
        "  predicted_class = np.array(predicted_class)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]] = {}\r\n",
        "  submission_dict[i[:-4]]['shape'] = predicted_class.shape\r\n",
        "  submission_dict[i[:-4]]['team'] = i.split('_')[0]\r\n",
        "  submission_dict[i[:-4]]['crop'] = i.split('_')[1][0].upper() + i.split('_')[1][1:]\r\n",
        "  submission_dict[i[:-4]]['segmentation'] = {}\r\n",
        "\r\n",
        "  rle_encoded_crop = rle_encode(predicted_class == 1)\r\n",
        "  rle_encoded_weed = rle_encode(predicted_class == 2)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]]['segmentation']['crop'] = rle_encoded_crop\r\n",
        "  submission_dict[i[:-4]]['segmentation']['weed'] = rle_encoded_weed\r\n",
        "\r\n",
        "# Exporting the submission_dict created into a submission.json file\r\n",
        "with open('/content/drive/My Drive/AN2DL/ImageSegmentation/submission_2.json', 'w') as f:\r\n",
        "    json.dump(submission_dict, f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eetgWI7hgV3J"
      },
      "source": [
        "# VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIrDBeWSgEfT"
      },
      "source": [
        "# Creating the CustomDataset objects for the model, followed by the actual dataset that will be used for segmentation\r\n",
        "# The images are preprocessed using the VGG-16 preprocess_input function\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "\r\n",
        "# Image dimensions to work with\r\n",
        "img_h = 2048\r\n",
        "img_w = 2048\r\n",
        "\r\n",
        "# Batch size\r\n",
        "bs = 4\r\n",
        "\r\n",
        "# Training\r\n",
        "dataset = CustomDataset('/content/Development_Dataset', 'training', \r\n",
        "                        img_generator=img_data_gen, mask_generator=mask_data_gen,\r\n",
        "                        preprocessing_function=preprocess_input, out_shape=[img_h, img_w])\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "\r\n",
        "train_dataset = train_dataset.batch(bs)                      \r\n",
        "\r\n",
        "# Validation\r\n",
        "dataset_valid = CustomDataset('/content/Development_Dataset', 'validation',\r\n",
        "                              preprocessing_function=preprocess_input, out_shape=[img_h, img_w])\r\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n",
        "                                               output_types=(tf.float32, tf.float32),\r\n",
        "                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n",
        "\r\n",
        "valid_dataset = valid_dataset.batch(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "553NrJXrFCZo"
      },
      "source": [
        "# Importing the pre-trained VGG-16 network excluding the top part\n",
        "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "# Setting the Fine-Tuning parameter\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 13\n",
        "    for layer in vgg.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    vgg.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvsdiF0TFTbt"
      },
      "source": [
        "# Defining the model structure\n",
        "# The encoder is composed by the imported VGG-16, while the decoder part is taken from the lab notebook\n",
        "# It is composed by an upsampling, a convolutional and a ReLU layers for each depth level\n",
        "# Finally, the prediction is done using 3 filters (as the number of classes) and a SoftMax activation function\n",
        "def create_model(depth, start_f, num_classes):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    # Encoder\n",
        "    model.add(vgg)\n",
        "    \n",
        "    start_f = 256\n",
        "        \n",
        "    # Decoder\n",
        "    for i in range(depth):\n",
        "        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f,\n",
        "                                         kernel_size=(3, 3),\n",
        "                                         strides=(1, 1),\n",
        "                                         padding='same'))\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "\n",
        "        start_f = start_f // 2\n",
        "\n",
        "    # Prediction Layer\n",
        "    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n",
        "                                     kernel_size=(1, 1),\n",
        "                                     strides=(1, 1),\n",
        "                                     padding='same',\n",
        "                                     activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k97OK6CRnFJS",
        "scrolled": true
      },
      "source": [
        "# Creating the model and visualizing a summary of it\n",
        "model_3 = create_model(depth=5, start_f=8, num_classes=3)\n",
        "\n",
        "# Visualize created model as a table\n",
        "model_3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MlmYGVMnFJW"
      },
      "source": [
        "# Optimization parameters\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "def meanIoU(y_true, y_pred):\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
        "\n",
        "    per_class_iou = []\n",
        "\n",
        "    for i in range(1,3): # Excluding the background class 0\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
        "    \n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "      per_class_iou.append(iou)\n",
        "\n",
        "    return tf.reduce_mean(per_class_iou)\n",
        "\n",
        "metrics = ['accuracy', meanIoU]\n",
        "\n",
        "# Compile Model\n",
        "model_3.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XiwaKZhnFJa",
        "scrolled": true
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"multiclass_segmentationexperiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"VGG-16_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model weights after each epoch inside the previously mentioned folder, only if the model improved in meanIoU on the Validation set.\n",
        "# Moreover, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "# Finally, the model is fitted using the training and validation data defined before.\n",
        "\n",
        "# Creating the \"multiclass_segmentation_experiments\" folder if not already created\n",
        "exps_dir = os.path.join(cwd, 'drive/My Drive/AN2DL/ImageSegmentation/', 'multiclass_segmentation_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model weights will be saved\n",
        "model_name = 'VGG-16'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "# Setting up the callback to save the model weights after each epoch only if there is an improvement in term of validation meanIoU \n",
        "callbacks_3 = []\n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, \n",
        "                                                   monitor='val_meanIoU',\n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=True)\n",
        "callbacks_3.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
        "    callbacks_3.append(es_callback)\n",
        "\n",
        "# Fitting the model\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier\n",
        "model_3.fit(x=train_dataset,\n",
        "           epochs=100,\n",
        "           steps_per_epoch=(len(dataset) // bs),\n",
        "           validation_data=valid_dataset,\n",
        "           validation_steps=(len(dataset_valid) // bs), \n",
        "           callbacks=callbacks_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JUMMkS9eyQW"
      },
      "source": [
        "# Loading the best weights of the trained model\r\n",
        "full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageSegmentation/multiclass_segmentation_experiments', exp_dir)\r\n",
        "latest = tf.train.latest_checkpoint(full_path)\r\n",
        "model_3.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo1JbCO5nFJh",
        "scrolled": false
      },
      "source": [
        "# Checking how the model predictions on the validation set\n",
        "import time\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Assigning a color to each class\n",
        "evenly_spaced_interval = np.linspace(0, 1, 20)\n",
        "colors = [cm.rainbow(x) for x in evenly_spaced_interval]\n",
        "\n",
        "iterator = iter(valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M75UjH7xxS7J"
      },
      "source": [
        "# Visualizing an image in the validation set, its mask and its model prediction\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 8))\n",
        "fig.show()\n",
        "image, target = next(iterator)\n",
        "\n",
        "image = image[0]\n",
        "target = target[0, ..., 0]\n",
        "\n",
        "out_sigmoid = model_3.predict(x=tf.expand_dims(image, 0))\n",
        "predicted_class = tf.argmax(out_sigmoid, -1)\n",
        "\n",
        "predicted_class = predicted_class[0, ...]\n",
        "\n",
        "target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "prediction_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "\n",
        "target_img[np.where(target == 0)] = [0, 0, 0]\n",
        "for i in range(1, 3):\n",
        "  target_img[np.where(target == i)] = np.array(colors[i-1])[:3] * 255\n",
        "\n",
        "prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\n",
        "for i in range(1, 3):\n",
        "  prediction_img[np.where(predicted_class == i)] = np.array(colors[i-1])[:3] * 255\n",
        "\n",
        "ax[0].imshow(np.uint8(image))\n",
        "ax[1].imshow(np.uint8(target_img))\n",
        "ax[2].imshow(np.uint8(prediction_img))\n",
        "\n",
        "fig.canvas.draw()\n",
        "time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYYxoSgbgxHD"
      },
      "source": [
        "# Computing the prediction on the Test_Dev images and generating the submission.json file\r\n",
        "submission_dict = {}\r\n",
        "\r\n",
        "# Storing the names of the images\r\n",
        "test_img_dir = os.path.join(test_dir, 'Images')\r\n",
        "file_names_img = os.listdir(test_img_dir)\r\n",
        "\r\n",
        "# For each image, we resize it to the dimensions on which the model has been trained and we apply the preprocess_input function on it\r\n",
        "# We give it as input to the model and then we resize the model output to match the original image size (according to the team it belongs)\r\n",
        "# Finally, we compute the argmax to get the predicted class for each pixel and we fill the submission_dict according to the requested structure and encoding\r\n",
        "for i in file_names_img:\r\n",
        "  image = Image.open(os.path.join(test_img_dir, i)).convert('RGB')\r\n",
        "  image = image.resize((img_h, img_w))\r\n",
        "  img_array = np.array(image)\r\n",
        "  img_array = preprocess_input(img_array)\r\n",
        "  out_softmax = model_3.predict(tf.expand_dims(img_array, 0))\r\n",
        "\r\n",
        "  if (i.split('_')[0] == 'Bipbip'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (1536, 2048), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Pead'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (2464, 3280), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Weedelec'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (3456, 5184), method='nearest')\r\n",
        "  elif (i.split('_')[0] == 'Roseau'):\r\n",
        "    resized_out_softmax = tf.image.resize(out_softmax, (820, 1225), method='nearest')\r\n",
        " \r\n",
        "  predicted_class = tf.argmax(resized_out_softmax, -1)\r\n",
        "  predicted_class = np.array(predicted_class)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]] = {}\r\n",
        "  submission_dict[i[:-4]]['shape'] = predicted_class.shape\r\n",
        "  submission_dict[i[:-4]]['team'] = i.split('_')[0]\r\n",
        "  submission_dict[i[:-4]]['crop'] = i.split('_')[1][0].upper() + i.split('_')[1][1:]\r\n",
        "  submission_dict[i[:-4]]['segmentation'] = {}\r\n",
        "\r\n",
        "  rle_encoded_crop = rle_encode(predicted_class == 1)\r\n",
        "  rle_encoded_weed = rle_encode(predicted_class == 2)\r\n",
        "\r\n",
        "  submission_dict[i[:-4]]['segmentation']['crop'] = rle_encoded_crop\r\n",
        "  submission_dict[i[:-4]]['segmentation']['weed'] = rle_encoded_weed\r\n",
        "\r\n",
        "# Exporting the submission_dict created into a submission.json file\r\n",
        "with open('/content/drive/My Drive/AN2DL/ImageSegmentation/submission_3.json', 'w') as f:\r\n",
        "    json.dump(submission_dict, f) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}